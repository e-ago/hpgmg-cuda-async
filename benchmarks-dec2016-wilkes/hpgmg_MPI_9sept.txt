CUDADRV_TAG=8.0-RC
CUDATK_TAG=8.0-RC
INFO: installing/picking peersync stuff from /home/hpcagos1/peersync
CUDA: /usr/local/Cluster-Apps/cuda/8.0-RC
CUDA_HOME: /usr/local/Cluster-Apps/cuda/8.0-RC
CUDA_PATH: /usr/local/Cluster-Apps/cuda/8.0-RC
CU_CPPFLAGS=-I/home/hpcagos1/peersync/src/cuda/361.74/include  -I/usr/local/Cluster-Apps/cuda/8.0-RC/include
CU_LDFLAGS= -L/usr/local/Cluster-Apps/cuda/8.0-RC/lib64 -L/usr/lib64
LD_LIBRARY_PATH=/usr/local/Cluster-Apps/cuda/8.0-RC/lib::/home/hpcagos1/peersync/lib:/usr/local/Cluster-Apps/cuda/8.0-RC/lib64::/usr/local/Cluster-Apps/cuda/8.0-RC/extras/CUPTI/lib64:/usr/local/Cluster-Apps/cuda/8.0-RC/extras/CUPTI/lib:/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/lib:/usr/local/Cluster-Apps/cuda/8.0-RC/lib64:/usr/local/Cluster-Apps/cuda/8.0-RC/lib:/usr/local/Cluster-Apps/intel/mkl/10.3.10.319/composer_xe_2011_sp1.10.319/mkl/lib/intel64:/usr/local/Cluster-Apps/global/lib:/usr/local/Cluster-Apps/vgl/2.3.1/64/lib:/usr/local/Cluster-Apps/slurm/lib:/home/hpcagos1/peersync/lib::/usr/local/Cluster-Apps/cuda/8.0-RC/extras/CUPTI/lib64:/usr/local/Cluster-Apps/cuda/8.0-RC/extras/CUPTI/lib:/usr/local/Cluster-Apps/cuda/8.0-RC/
Configuration complete in: /home/hpcagos1/peersync/src/hpgmg/build
To build: make -j3 -C build
make: Entering directory `/home/hpcagos1/peersync/src/hpgmg/build'
rm -rf obj lib bin
make: Leaving directory `/home/hpcagos1/peersync/src/hpgmg/build'
make: Entering directory `/home/hpcagos1/peersync/src/hpgmg/build'
/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/bin/mpicc -c -std=c99  -DUSE_MPI=1 -DUSE_BICGSTAB=1 -DUSE_SUBCOMM=1 -DUSE_FCYCLES=1 -DUSE_GSRB=1 -O2 -fopenmp -DBLOCKCOPY_TILE_I=32 -DBLOCKCOPY_TILE_J=4 -DBLOCKCOPY_TILE_K=8 -DBOUNDARY_TILE_I=64 -DBOUNDARY_TILE_J=16 -DBOUNDARY_TILE_K=16 -DMAX_SOLVES=100 -DCUDA_UM_ALLOC -DCUDA_UM_ZERO_COPY -DUSE_REG -DUSE_TEX -DMPICH_IGNORE_CXX_SEEK -DMPICH_SKIP_MPICXX -DENABLE_EXCHANGE_BOUNDARY_COMM=1 -DENABLE_RESTRICTION_COMM=1 -DENABLE_RESTRICTION_ASYNC=1 -DENABLE_INTERPOLATION_COMM=1 -DENABLE_INTERPOLATION_ASYNC=1 -DPROFILE_NVTX_RANGES -I/home/hpcagos1/peersync/include -I/home/hpcagos1/peersync/src/cuda/361.74/include  -I/usr/local/Cluster-Apps/cuda/8.0-RC/include   -I/usr/local/Cluster-Apps/cuda/8.0-RC/bin/../include -I/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/bin/../include -MMD -MP /home/hpcagos1/peersync/src/hpgmg/finite-volume/source/debug.c -o obj/finite-volume/source/debug.o
/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/bin/mpicc -c -std=c99  -DUSE_MPI=1 -DUSE_BICGSTAB=1 -DUSE_SUBCOMM=1 -DUSE_FCYCLES=1 -DUSE_GSRB=1 -O2 -fopenmp -DBLOCKCOPY_TILE_I=32 -DBLOCKCOPY_TILE_J=4 -DBLOCKCOPY_TILE_K=8 -DBOUNDARY_TILE_I=64 -DBOUNDARY_TILE_J=16 -DBOUNDARY_TILE_K=16 -DMAX_SOLVES=100 -DCUDA_UM_ALLOC -DCUDA_UM_ZERO_COPY -DUSE_REG -DUSE_TEX -DMPICH_IGNORE_CXX_SEEK -DMPICH_SKIP_MPICXX -DENABLE_EXCHANGE_BOUNDARY_COMM=1 -DENABLE_RESTRICTION_COMM=1 -DENABLE_RESTRICTION_ASYNC=1 -DENABLE_INTERPOLATION_COMM=1 -DENABLE_INTERPOLATION_ASYNC=1 -DPROFILE_NVTX_RANGES -I/home/hpcagos1/peersync/include -I/home/hpcagos1/peersync/src/cuda/361.74/include  -I/usr/local/Cluster-Apps/cuda/8.0-RC/include   -I/usr/local/Cluster-Apps/cuda/8.0-RC/bin/../include -I/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/bin/../include -MMD -MP /home/hpcagos1/peersync/src/hpgmg/finite-volume/source/timers.c -o obj/finite-volume/source/timers.o
/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/bin/mpicc -c -std=c99  -DUSE_MPI=1 -DUSE_BICGSTAB=1 -DUSE_SUBCOMM=1 -DUSE_FCYCLES=1 -DUSE_GSRB=1 -O2 -fopenmp -DBLOCKCOPY_TILE_I=32 -DBLOCKCOPY_TILE_J=4 -DBLOCKCOPY_TILE_K=8 -DBOUNDARY_TILE_I=64 -DBOUNDARY_TILE_J=16 -DBOUNDARY_TILE_K=16 -DMAX_SOLVES=100 -DCUDA_UM_ALLOC -DCUDA_UM_ZERO_COPY -DUSE_REG -DUSE_TEX -DMPICH_IGNORE_CXX_SEEK -DMPICH_SKIP_MPICXX -DENABLE_EXCHANGE_BOUNDARY_COMM=1 -DENABLE_RESTRICTION_COMM=1 -DENABLE_RESTRICTION_ASYNC=1 -DENABLE_INTERPOLATION_COMM=1 -DENABLE_INTERPOLATION_ASYNC=1 -DPROFILE_NVTX_RANGES -I/home/hpcagos1/peersync/include -I/home/hpcagos1/peersync/src/cuda/361.74/include  -I/usr/local/Cluster-Apps/cuda/8.0-RC/include   -I/usr/local/Cluster-Apps/cuda/8.0-RC/bin/../include -I/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/bin/../include -MMD -MP /home/hpcagos1/peersync/src/hpgmg/finite-volume/source/level.c -o obj/finite-volume/source/level.o
/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/bin/mpicc -c -std=c99  -DUSE_MPI=1 -DUSE_BICGSTAB=1 -DUSE_SUBCOMM=1 -DUSE_FCYCLES=1 -DUSE_GSRB=1 -O2 -fopenmp -DBLOCKCOPY_TILE_I=32 -DBLOCKCOPY_TILE_J=4 -DBLOCKCOPY_TILE_K=8 -DBOUNDARY_TILE_I=64 -DBOUNDARY_TILE_J=16 -DBOUNDARY_TILE_K=16 -DMAX_SOLVES=100 -DCUDA_UM_ALLOC -DCUDA_UM_ZERO_COPY -DUSE_REG -DUSE_TEX -DMPICH_IGNORE_CXX_SEEK -DMPICH_SKIP_MPICXX -DENABLE_EXCHANGE_BOUNDARY_COMM=1 -DENABLE_RESTRICTION_COMM=1 -DENABLE_RESTRICTION_ASYNC=1 -DENABLE_INTERPOLATION_COMM=1 -DENABLE_INTERPOLATION_ASYNC=1 -DPROFILE_NVTX_RANGES -I/home/hpcagos1/peersync/include -I/home/hpcagos1/peersync/src/cuda/361.74/include  -I/usr/local/Cluster-Apps/cuda/8.0-RC/include   -I/usr/local/Cluster-Apps/cuda/8.0-RC/bin/../include -I/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/bin/../include -MMD -MP /home/hpcagos1/peersync/src/hpgmg/finite-volume/source/operators.fv4.c -o obj/finite-volume/source/operators.fv4.o
/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/bin/mpicc -c -std=c99  -DUSE_MPI=1 -DUSE_BICGSTAB=1 -DUSE_SUBCOMM=1 -DUSE_FCYCLES=1 -DUSE_GSRB=1 -O2 -fopenmp -DBLOCKCOPY_TILE_I=32 -DBLOCKCOPY_TILE_J=4 -DBLOCKCOPY_TILE_K=8 -DBOUNDARY_TILE_I=64 -DBOUNDARY_TILE_J=16 -DBOUNDARY_TILE_K=16 -DMAX_SOLVES=100 -DCUDA_UM_ALLOC -DCUDA_UM_ZERO_COPY -DUSE_REG -DUSE_TEX -DMPICH_IGNORE_CXX_SEEK -DMPICH_SKIP_MPICXX -DENABLE_EXCHANGE_BOUNDARY_COMM=1 -DENABLE_RESTRICTION_COMM=1 -DENABLE_RESTRICTION_ASYNC=1 -DENABLE_INTERPOLATION_COMM=1 -DENABLE_INTERPOLATION_ASYNC=1 -DPROFILE_NVTX_RANGES -I/home/hpcagos1/peersync/include -I/home/hpcagos1/peersync/src/cuda/361.74/include  -I/usr/local/Cluster-Apps/cuda/8.0-RC/include   -I/usr/local/Cluster-Apps/cuda/8.0-RC/bin/../include -I/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/bin/../include -MMD -MP /home/hpcagos1/peersync/src/hpgmg/finite-volume/source/mg.c -o obj/finite-volume/source/mg.o
/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/bin/mpicc -c -std=c99  -DUSE_MPI=1 -DUSE_BICGSTAB=1 -DUSE_SUBCOMM=1 -DUSE_FCYCLES=1 -DUSE_GSRB=1 -O2 -fopenmp -DBLOCKCOPY_TILE_I=32 -DBLOCKCOPY_TILE_J=4 -DBLOCKCOPY_TILE_K=8 -DBOUNDARY_TILE_I=64 -DBOUNDARY_TILE_J=16 -DBOUNDARY_TILE_K=16 -DMAX_SOLVES=100 -DCUDA_UM_ALLOC -DCUDA_UM_ZERO_COPY -DUSE_REG -DUSE_TEX -DMPICH_IGNORE_CXX_SEEK -DMPICH_SKIP_MPICXX -DENABLE_EXCHANGE_BOUNDARY_COMM=1 -DENABLE_RESTRICTION_COMM=1 -DENABLE_RESTRICTION_ASYNC=1 -DENABLE_INTERPOLATION_COMM=1 -DENABLE_INTERPOLATION_ASYNC=1 -DPROFILE_NVTX_RANGES -I/home/hpcagos1/peersync/include -I/home/hpcagos1/peersync/src/cuda/361.74/include  -I/usr/local/Cluster-Apps/cuda/8.0-RC/include   -I/usr/local/Cluster-Apps/cuda/8.0-RC/bin/../include -I/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/bin/../include -MMD -MP /home/hpcagos1/peersync/src/hpgmg/finite-volume/source/solvers.c -o obj/finite-volume/source/solvers.o
/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/bin/mpicc -c -std=c99  -DUSE_MPI=1 -DUSE_BICGSTAB=1 -DUSE_SUBCOMM=1 -DUSE_FCYCLES=1 -DUSE_GSRB=1 -O2 -fopenmp -DBLOCKCOPY_TILE_I=32 -DBLOCKCOPY_TILE_J=4 -DBLOCKCOPY_TILE_K=8 -DBOUNDARY_TILE_I=64 -DBOUNDARY_TILE_J=16 -DBOUNDARY_TILE_K=16 -DMAX_SOLVES=100 -DCUDA_UM_ALLOC -DCUDA_UM_ZERO_COPY -DUSE_REG -DUSE_TEX -DMPICH_IGNORE_CXX_SEEK -DMPICH_SKIP_MPICXX -DENABLE_EXCHANGE_BOUNDARY_COMM=1 -DENABLE_RESTRICTION_COMM=1 -DENABLE_RESTRICTION_ASYNC=1 -DENABLE_INTERPOLATION_COMM=1 -DENABLE_INTERPOLATION_ASYNC=1 -DPROFILE_NVTX_RANGES -I/home/hpcagos1/peersync/include -I/home/hpcagos1/peersync/src/cuda/361.74/include  -I/usr/local/Cluster-Apps/cuda/8.0-RC/include   -I/usr/local/Cluster-Apps/cuda/8.0-RC/bin/../include -I/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/bin/../include -MMD -MP /home/hpcagos1/peersync/src/hpgmg/finite-volume/source/hpgmg-fv.c -o obj/finite-volume/source/hpgmg-fv.o
/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/bin/mpic++ -c  -DUSE_MPI=1 -DUSE_BICGSTAB=1 -DUSE_SUBCOMM=1 -DUSE_FCYCLES=1 -DUSE_GSRB=1 -O2 -DBLOCKCOPY_TILE_I=32 -DBLOCKCOPY_TILE_J=4 -DBLOCKCOPY_TILE_K=8 -DBOUNDARY_TILE_I=64 -DBOUNDARY_TILE_J=16 -DBOUNDARY_TILE_K=16 -DMAX_SOLVES=100 -DCUDA_UM_ALLOC -DCUDA_UM_ZERO_COPY -DUSE_REG -DUSE_TEX -DMPICH_IGNORE_CXX_SEEK -DMPICH_SKIP_MPICXX -DENABLE_EXCHANGE_BOUNDARY_COMM=1 -DENABLE_RESTRICTION_COMM=1 -DENABLE_RESTRICTION_ASYNC=1 -DENABLE_INTERPOLATION_COMM=1 -DENABLE_INTERPOLATION_ASYNC=1 -DPROFILE_NVTX_RANGES -I/home/hpcagos1/peersync/include -I/home/hpcagos1/peersync/src/cuda/361.74/include  -I/usr/local/Cluster-Apps/cuda/8.0-RC/include   -I/usr/local/Cluster-Apps/cuda/8.0-RC/bin/../include -I/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/bin/../include -MMD -MP /home/hpcagos1/peersync/src/hpgmg/finite-volume/source/comm.cc -o obj/finite-volume/source/comm.o
/usr/local/Cluster-Apps/cuda/8.0-RC/bin/nvcc -c -DUSE_MPI=1 -DUSE_BICGSTAB=1 -DUSE_SUBCOMM=1 -DUSE_FCYCLES=1 -DUSE_GSRB=1 -I/usr/local/Cluster-Apps/cuda/8.0-RC/bin/../include -I/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/bin/../include -O2 -lineinfo -DBLOCKCOPY_TILE_I=32 -DBLOCKCOPY_TILE_J=4 -DBLOCKCOPY_TILE_K=8 -DBOUNDARY_TILE_I=64 -DBOUNDARY_TILE_J=16 -DBOUNDARY_TILE_K=16 -DMAX_SOLVES=100 -DCUDA_UM_ALLOC -DCUDA_UM_ZERO_COPY -DUSE_REG -DUSE_TEX -DMPICH_IGNORE_CXX_SEEK -DMPICH_SKIP_MPICXX -DENABLE_EXCHANGE_BOUNDARY_COMM=1 -DENABLE_RESTRICTION_COMM=1 -DENABLE_RESTRICTION_ASYNC=1 -DENABLE_INTERPOLATION_COMM=1 -DENABLE_INTERPOLATION_ASYNC=1 -DPROFILE_NVTX_RANGES -I/home/hpcagos1/peersync/include -I/home/hpcagos1/peersync/src/cuda/361.74/include  -I/usr/local/Cluster-Apps/cuda/8.0-RC/include   -gencode code=sm_35,arch=compute_35   /home/hpcagos1/peersync/src/hpgmg/finite-volume/source/cuda/operators.fv4.cu -o obj/finite-volume/source/cuda/operators.fv4.cu.o
nvcc -dlink obj/finite-volume/source/cuda/operators.fv4.cu.o -gencode code=sm_35,arch=compute_35   -o obj/finite-volume/source/cuda/operators.fv4.cu.o.dlink -L/usr/local/Cluster-Apps/cuda/8.0-RC/lib64 -L/usr/lib64 -L/home/hpcagos1/peersync/lib  -lmp -lgdsync -lgdrapi -lcuda -libverbs 
/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/bin/mpic++ -O2 -fopenmp -DBLOCKCOPY_TILE_I=32 -DBLOCKCOPY_TILE_J=4 -DBLOCKCOPY_TILE_K=8 -DBOUNDARY_TILE_I=64 -DBOUNDARY_TILE_J=16 -DBOUNDARY_TILE_K=16 -DMAX_SOLVES=100 -DCUDA_UM_ALLOC -DCUDA_UM_ZERO_COPY -DUSE_REG -DUSE_TEX -DMPICH_IGNORE_CXX_SEEK -DMPICH_SKIP_MPICXX -DENABLE_EXCHANGE_BOUNDARY_COMM=1 -DENABLE_RESTRICTION_COMM=1 -DENABLE_RESTRICTION_ASYNC=1 -DENABLE_INTERPOLATION_COMM=1 -DENABLE_INTERPOLATION_ASYNC=1 -DPROFILE_NVTX_RANGES -I/home/hpcagos1/peersync/include -I/home/hpcagos1/peersync/src/cuda/361.74/include  -I/usr/local/Cluster-Apps/cuda/8.0-RC/include   -I/usr/local/Cluster-Apps/cuda/8.0-RC/bin/../include -I/usr/local/Cluster-Apps/openmpi/gnu/1.10.3/bin/../include -L/usr/local/Cluster-Apps/cuda/8.0-RC/lib64 -L/usr/lib64 -L/home/hpcagos1/peersync/lib   -o bin/hpgmg-fv obj/finite-volume/source/debug.o obj/finite-volume/source/timers.o obj/finite-volume/source/level.o obj/finite-volume/source/operators.fv4.o obj/finite-volume/source/mg.o obj/finite-volume/source/solvers.o obj/finite-volume/source/hpgmg-fv.o obj/finite-volume/source/comm.o obj/finite-volume/source/cuda/operators.fv4.cu.o obj/finite-volume/source/cuda/operators.fv4.cu.o.dlink -lmp -lgdsync -lgdrapi -lcuda -libverbs  -L/usr/local/Cluster-Apps/cuda/8.0-RC/bin/../lib64 -lcudart -lnvToolsExt -lstdc++ -lm
make: Leaving directory `/home/hpcagos1/peersync/src/hpgmg/build'
MODE: MPI, SIZE: 4, PROC: 2
use cuda                           1            0            0            0            0   
Total by level                0.006236     0.002243     0.000716     0.000261     0.000082     0.009538
MODE: MPI, SIZE: 4, PROC: 4
use cuda                           1            0            0            0            0   
Total by level                0.007465     0.003674     0.001441     0.000858     0.000212     0.013649
MODE: MPI, SIZE: 4, PROC: 8
use cuda                           1            0            0            0            0            0   
Total by level                0.007887     0.004205     0.001424     0.000921     0.000327     0.000100     0.014864
MODE: MPI, SIZE: 4, PROC: 16
use cuda                           1            0            0            0            0   
Total by level                0.008726     0.004943     0.002350     0.002098     0.000801     0.018918
MODE: MPI, SIZE: 5, PROC: 2
use cuda                           1            1            0            0            0            0   
Total by level                0.012118     0.016020     0.003249     0.000929     0.000331     0.000101     0.032748
MODE: MPI, SIZE: 5, PROC: 4
use cuda                           1            1            0            0            0            0   
Total by level                0.018596     0.022325     0.005439     0.001920     0.001053     0.000273     0.049607
MODE: MPI, SIZE: 5, PROC: 8
use cuda                           1            1            0            0            0            0            0   
Total by level                0.019190     0.023703     0.006240     0.001737     0.001156     0.000394     0.000120     0.052540
MODE: MPI, SIZE: 5, PROC: 16
use cuda                           1            1            0            0            0            0   
Total by level                0.022479     0.027548     0.007490     0.003177     0.002602     0.000947     0.064244
MODE: MPI, SIZE: 6, PROC: 2
use cuda                           1            1            1            0            0            0            0   
Total by level                0.045390     0.051828     0.066217     0.003989     0.001139     0.000393     0.000121     0.169077
MODE: MPI, SIZE: 6, PROC: 4
use cuda                           1            1            1            0            0            0            0   
Total by level                0.079664     0.085944     0.106565     0.007108     0.002351     0.001278     0.000295     0.283206
MODE: MPI, SIZE: 6, PROC: 8
use cuda                           1            1            1            0            0            0            0            0   
Total by level                0.085691     0.094267     0.118898     0.008267     0.002146     0.001367     0.000454     0.000139     0.311230
MODE: MPI, SIZE: 6, PROC: 16
slurmstepd: *** JOB 3468833 CANCELLED AT 2016-09-09T15:46:01 *** on tesla110
